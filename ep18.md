http://bananasandlenses.net/episode018

РЧ: Доброго времени суток, вы слушаете 18 выпуск подкаста бананы и линзы. С вами сегодя Денис Редозубов

ДР: Привет

РЧ: Денис Шевченко

ДШ: Приветствую

РЧ: и я, Роман Чепляка. Первая тема у нас сегодня посвящена расширению Strict, которое мы обсуждали в предыдущих выпусках. Это расширение, которое позволяет анотировать модуль и код в этом модуле будет компилироваться как строгий код. Это, по крайней мере, идея. Были спекуляции на тему того, что это расширение сделает Хаскель более простым для новичков, которым не хочется разбираться в тонкостях ленивых вычислений. Однако, как выяснилось, чтобы разбираться, что же делает эта прагма Strict, даже не все авторы компилятора это понимают. И вот, на этой неделе, было письмо в рассылку ghc-devs от Омера Синана Агасана, как-то так, который интересуется, а почему ... Значит, он написал функцию zip, которая деконструирует списочные аргументы на head и tail и вот эти head и tail, несмотря на то, что модуль анотирован, как Strict, они строго не вычисляются. То есть `zip (h:hs) (y:ys)` и вот эти `x` и `y` продолжают быть санками(thunk). Он спрашивает: "как же так и почему?". И получилась довольно интересная дискуссия, в которой было несколько версий, как же всё-таки должен работать Strict. То есть в первом приближении всё понятно и все были согласны, что Strict должен всё форсировать, но когда стали углубляться в детали, стало понятно, что ничего не понятно. Я предложил своё видение этого, которое в чём-то основано на другом, похожем расширении - это [UnliftedDataTypes](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes). Я напомню, UnliftedDataTypes делает некоторые типы unlifted и это означает, что все байндинги этого типа автоматически вычисляются. То есть мы работаем в терминах именно байндингов, в терминах переменных, и мне казалось эта концепция хорошо переносится сюда. Вот я предложил ... Тут что важно, важно, может быть, даже не то, какая именно будет семантика, важно чтобы она была простой. Чтобы можно было любому человеку объяснить в двух словах что делает прагма Strict. И моя попытка объяснить была именно такой, что если у нас есть модуль Strict, то все переменные, которые там определены, будут автоматически вычислены. Ну и, наверное, стоило бы добавить, что и function applications тоже будут автоматически вычислены. Хотя, может быть, это и следует. Да, главная идея такая, что любое определение будет строго вычислено. С другой стороны, Саймон Пейтон Джонс предложил своё определение. Определение Саймона тоже на первый взгляд простое и понятное. Саймон говорит, что код, который скомпилирован со Strict, не будет создавать санков. То есть он определяет это не в терминах такой денотационной семантики и не в терминах синтаксических, а в терминах операционных. Код, как он будет работать? Он будет работать каким-то таким образом, чтобы не создавать санков. Но там есть проблема с определением Саймона. Я, по крайней мере, 2 проблемы нашёл и Саймон со мной согласился и нашёл ещё третью проблему в собственном определении.

Во-первых, что будет, если у нас какой-то байндинг не используется? Тут ещё надо сказать, вот мы использовали аналогию с UnliftedDataTypes, ещё есть аналогия с [Bang Patterns](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/bang-patterns.html). То есть интуитивно в модуле Strict у нас все patterns с восклицательными знаками. И что будет, если мы, например, скажем, что `let _ = чему-то-undefined in что-то`. Вот будет это `undefined` вычислено или нет? По аналогии с `bang patterns` он будет вычислен даже несмотря на то, что он не используется. Но, по определению Саймона, полностью легально его просто выкинуть и не вычислять, потому что коль скоро мы не создаём санков, то всё хорошо. Но понятно, что это не очень согласуется с идеей строгого вычисления всего и, естественно, семантика у этих двух вычислений очень разная, потому что в одном случае мы получаем исключение, в другом случае мы исключения не получаем.

Другой пример, который я привёл, это `f g = g undefined`. То есть мы вызываем какую-то функцию с аргументом `undefined`. По логике, коль скоро наш модуль Strict, то эта аппликация должна быть строгой, то есть должно получиться исключение. Даже если `g` это какой-нибудь `const`, который аргумент свой не вычисляет, но коль скором мы в строгом модуле передаём в функцию `undefined`, то должно получиться исключение. Правда же. То есть это применение должно быть строгим. Но, поскольку `undefined` это не санк, это некий CAF([constant applicative form](https://wiki.haskell.org/Constant_applicative_form)), который определён... Вернее, это санк, но он определён в другом модуле. То есть, может быть, изначально он что-то типа санка, но потом он заменён исключением и каждый раз, когда мы обращаемся к `undefined` мы получаем то самое исключение. Как-то так. Ну суть важно. Главное, что `undefined` для наших целей - это указатель. То есть это на санк, который мы создаём, идея Саймона была в том, что мы не создаём новых санков, но это уже CAF, который уже создан. Мы же его не создаём, мы его просто передаём и поэтому тоже легально его не вычислять в этом контексте. Потом Саймон сам задаётся вопросом - а действительно, что же будет, например, если мы просто вызываем что-то типа `f (g x)`, где `f` определена в ленивом модуле? То есть будет ли эта аппликация ленивой или строгой? То есть с одной стороны она должна быть ленивой, поскольку `f` определена в ленивом модуле, с другой стороны, она должна быть строгой, потому что `f (g x)` определена в строгом модуле. В общем ничего не понятно с этой прагмой.

ДШ: Сами себе проблемы сделали, теперь гадают, как же мы это разрулим.

РЧ: Да, да. Типа того. Вот авторы этой прагмы, это Йохан Тибль и Адам Сендберг Эриксон. И я так понимаю, что это был Google Summer of Code проект Адама и вот они тоже, как бы, над этим думают. Йохан отписался, что у него мало времени, чтобы над всем этим думать. И сейчас, поскольку это уже должно войти в GHC 8.0, который вот-вот, совсем скоро, должен быть выпущен, то консенсуc такой, что давайте мы попробуем задокументировать как-то что же мы тут наделали и как оно должно себя вести, а дальше будем думать.

ДР: У меня не до конца срастается ещё, как эта прагма должна работать с какими-нибудь бесконечными структурами, например?

РЧ: Ну, там проще, потому что мы только говорим, что это Weak Head Normal Form. То есть оно не полностью вычисляется, оно вычисляется только до WHNF. Вопрос в том, что именно будет вычисляться до WHNF? Потому что, если, например, у нас есть сложные вложенные patterns, то будет ли только верхний уровень вычислен до WHNF или будут все отдельные переменные вычислены до WHNF? 

ДР: А когда ты работаешь с бесконечным списком, как про это думать надо?

РЧ: Когда ты работаешь с бесконечным списком, ты не можешь его заматчить бесконечно.

ДР: Но если ты ожидаешь, что код будет строго исполняться, если мы говорим, что можем рассуждать про это, как про операционную семантику, не про денотационную, то как-то этот кейс не клеится. Особенно в рамках того, что ты говоришь, что все хотят иметь объяснение, когда можно на двух пальцах объяснить человеку, который не знаком с этой прагмой, что она вообще делает. Тут такие кейсы возникают. Ты только назвал 3 штуки и всё равно возникает куча вопросов.

РЧ: Кстати да, насчёт бесконечных списков, мне даже сложно сказать, как оно должно быть. Но, возможно, у тебя даже не получится сконструировать бесконечный список в строгом модуле. Другое дело, что ты всегда можешь импортировать его из ленивого модуля.

ДР: Ну да. А тут другой вопрос поднимается. Если мы хотим утвержадать, что в этом модуле всё Strict, мы, это спекуляция, конечно, но, возможно, мы не можем депендится на функции, которые определены в нестрогих модулях.

РЧ: Не-не, однозначно можем. Это одно из фундаментальных требований. Иначе всё вообще идёт к чертям, потому что Base выбрасываем и так далее.

ДР: Да, про это было бы очень просто рассуждать, если бы там был какой-то контроль изоляции, можно было бы сказать, что вот это мы юзаем только с Base или только с чем-то, есть минимальные какие-то определения, можете использовать их, импортить в других модулях, но не наоборот. Просто это было бы более понятно с точки зрения какого-то опыта работы...

РЧ: Ну, оно было бы более понятно, но менее полезно. Тут, всё-таки, акцент на прагматичность.

ДР: Полезность, да.

ДШ: Честно говоря, смотрю со стороны и получилось так - сделали идею, которая породила больше потенциальных проблем и обсуждений, нежели принесла какую-либо пользу. Вам не кажется?

РЧ: Она не то чтобы породила. Тебя же никто не заставляет эту прагму всключать и коль скоро ты её не включаешь, у тебя никаких проблем нет. Поэтому это не тот случай, когда из-за того, что не добавили фичу, что-то другое сломалось. Просто вопрос в том, как сама эта фича должна работать. В GHC 8.0 она экспериментальная будет. Если выяснится, что она должна работать как-то по-другому или даже, что это вообще была плохая идея, давайте её выкинем, то, в общем, никто не мешает её взять и выкинуть.

ДР: Ну и Йохан в общем-то открытым текстом писал, что это интересно, как такая ресёрч фича. То есть мы не знаем, как это использовать, но мы бы хотели найти способ.

Так как у нас подкаст про Stack, то я немного про Стек расскажу. Вот обновил я Стек и там появилась [поддержка Nix](http://docs.haskellstack.org/en/stable/nix_integration.html). Я даже пробовал это использовать и сейчас хочу вам рассказать, как я думал это использовать и, может быть, кто-то из вас подкинет мне идею, как это можно развивать и куда это должно в конце-концов прийти.

ДШ: Да, да. Это весьма интересно, расскажи, пожалуйста!

ДР: Смотри, у нас есть Стек, у нас есть Никс. Никс позволяет нам делать строго больше, чем Стек. Он позволяет нам депендиться на какие-то вещи за пределами хаскельной инфраструктуры, а Стек позволяет нам только менеджить библиотеки, проекты из нескольких cabal-проектов и версию компилятора. Если включить в Стеке поддержку Никса, вы можете добавлять в реквайрементсы названия Никс-пакетов или как оно там называется, деревейшн. Я буду говорить пакетов. Вы можете описывать такие ситуации, что для сборки вот этого проекта на Хаскеле Стеком, мне нужен ещё Перл. Ну, потому что вы в тестах, там, внезапно вызываете Перл для того, чтобы использовать, ну я не знаю, странная у вас конструкция такая.

РЧ: Да, на самом деле Перл в хаскельных проектах незаменим. И я его пару раз использовал, как дешёвую замену Темплейт Хаскелю. С Темплейт Хаскелем надо каждый раз идти, читать документацию, какие там конструкторы, как их применять, потом его немножко отлаживать. А тут, когда кучу кода надо нагенерировать, я пишу за полминуты какой-нибудь перловый скрипт, который генерирует 2 экрана кода, запихиваю его в git и всё круто.

ДР: А у нас другая история была. У нас, например, тесты депендятся на Ruby, потому что библиотека для работы с последним драфтом JSON Schema, она называется [hjsonschema](https://github.com/seagreen/hjsonschema), она нормально скопы не умеет рулить. И я залез туда внутрь, начал писать патчик и что-то быстро не осилил. Потом вопрос снова поднялся, залез коллега, посмотрел внутрь и говорит: "быстро не сделать". Мы взяли воткнули в тесты просто вызов рубишный, который готовыми библиотеками валидирует ту же джейсон-схему. Так у нас появился депенденси в тестах.

РЧ: Я только одного не понимаю. Ты рекламируешь Стек, как решение для внешних зависимостей? Мне казалось, что Никс наоборот, гораздо больше для этого подходит.

ДР: Да, я не дорассказал, я только проблематику сейчас озвучил. В Никсе пявился флажочек новый и опции, которые можно в конфиге указать. Флажочек `--nix` и он включает поддержку Никс и пытается поставить все пэккеджи, которые у вас там указаны. Таким образом, если у вас тесты внезапно, в каком-то месте зовут перлы или Руби, вы сможете эту ситуацию описать и, если на машине, где вы делаете `stack build` или `stack test` у вас настроен Nix, то Стек автоматически из Никса подтянет все эти зависимости и сделает hopefully правильную вещь. Но это минимальный кейс и на его основе хотелось бы построить что-нибудь покруче. Потому что в Никсе есть поддержка хаскельных пакетов и она уже давно, довольно взрослая. По крайней мере, так позиционируется.

ДШ: Более того, Денис, прошу прощения, я добавлю, что в Никсе не только есть поддержка хаскельной инфраструктуры, скажем так, а там ещё и поддежка [LTS](http://www.stackage.org/lts).

ДР: Вот, я к этому как раз веду, потому что в Никсе вы можете найти любой LTS Stackage и таким образом вы может на халяву получить бинарный кэш. То есть, если вы используете Stackage, вам нужно скачать библиотеки и потом их компилять упорно и до победы. А если вы используете этот же снэпшот LTS из Никса, вам достаточно будет скачать просто готовые эти пакетики. Естественно тут проблема, где как раз уже нужен Стек, это когда вы не готовы свой проект взять и резко перевести на Никс. Потому что у вас есть какие-то зависимости, которых нет в Никс-пэккеджес, ну, версии библиотек, например. Или вы депендетесь на какие-то форки библиотек, вам нужно, например, с Гитхаба что-то взять.

РЧ: А разве Никс это не может сделать? Ты же можешь сделать свой собственный пакет, добавить в Никс.

ДР: Да, ты можешь это сделать, но для того, чтобы это сделать, тебе нужно с Никсом разобраться. А я говорю про ситуацию, когда ты можешь плавно себе что-то улучшить. По крайне мере, что я думаю сделать. Я думаю максимум зависимостей перевести на LTS, чего, например, у меня на работе не сделано. Это просто сократит время компиляции и позволит бинарный кэш из Никса использовать. А всё остальное, пока что, на следующем этапе, видимо, будет Стеком тянуться. Просто потому, что Никс он своеобразен, он требует времени для того, чтобы всё это ввернуть. И насколько я знаю, там нет способа это лучше сделать, кроме как мейнтейнить собственный форк Никс-пэккеджес, что может быть проблемно и времезатратно, если вы хотите это всё время поддерживать.

РЧ: Знаешь, Денис, что мне это напоминает? Напоминает ту же логику, что я хочу писать на Хаскеле, но поскольку весь Хаскель это сильно сложно, то я включу себе `-XStrict` и постепенно буду Хаскель изучать. Сначала без ленивости, а потом и ленивость. Вот примерно то же ты говоришь, потому что, я думаю, если ты будешь пытаться использовать Никс и Стек вместе, то ты огребёшь гораздо больше проблем и тебе придётся гораздо глубже копаться в Никсе, чтобы их пофиксить, чем если бы ты просто использовал Никс. Потому что Никс - это уже некая проторенная дорожка. Много там всяких постов, есть там какой-нибудь IRC канал, наверное, где ты можешь спросить "почему у меня это не работает". А тут ты будешь постоянно огребать какие-то непонятные проблемы, с которым ты идёшь к никсовым разработчикам, а они говорят "это ваш непонятный Стек чего-то не то делает". Ты идёшь в Стек, а они говорят "окей, мы пофиксим это в следующем релизе" и так далее.

ДР: Но тут есть момент, про который я не рассказал. В Никсе вы можете описать, как ваш пакет нужно собирать на собственном никсовом языке. В Стеке вы можете указать вот этот файлик, где вы всё описали. Поэтому я думаю, что можно использовать схему, где как раз очень-очень тонкая прослойка Стека это использует. Причём настолько тонкая, что вы Стек, вероятно, можете после этого одним дуновением ветра выбросить. Поэтому здесь есть, воможно, какой-то middleground, если вы хотите в эту сторону двигаться. Я пока стабильную почву под ногами не нашёл на эту тему, но хотелось бы попробовать в будущем.

РЧ: Из того, что ты рассказал, мне представляется, что эта фича не для тех, кто использует Никс в первую очередь, а для тех, кто использует Стек. Если вы используете Стек и вам надо что-то левое поставить, то можете, допустим, в добавок, принести себе Никс и использовать его, как помощник Стеку. Но, если вы в такой ситуации, как, например, Шевченко у нас, у которого всё хозяйство построено на Никс, то нести туда Стек, наверное, не очень осмысленно.

ДШ: Вообще бессмысленно.

РЧ: С другой стороны, я же слышал все эти жалобы от пользователей Никс, что когда ты просто компилируешь пакеты, то всё окей. Но когда ты должен их разрабатывать и, там, тебе нужен тот же GHCi, тебе надо что-то там постоянно перекомпилировать, то, вроде как, Никс для этого не очень удобен и они всё равно используют cabal для которого Стек является лучшей альтернативой.

ДР: Подожди. Кабал ты не можешь исключить из уравнения вообще.

РЧ: Я имею в виду cabal-install, как программу.

ДР: Тебе не нужно его исключать, Никс всё равно его использует, потому что Никсу чем-то нужно собирать хаскельные проекты. А для разработки там используется щтука под названием Nix-shell. Она представляет собой следующее - вы описываете что вы хотите видеть на языке Никс в своём окружении и потом говорите, там... То есть, например, из манифеста Кабала, который точка-кабал, можно сгенерировать автоматически утилитой cabal2nix файл с расширением nix. Который будет все пакеты просто перечислять и попытается их поставить Никсом. И вы можете shell открыть в этом окружении. Идея в том, что вы попадаете в окружение, где у вас есть только всё нужное и нет ничего ненужного. И если в этом окружении сказать `cabal build` оно должно работать, потому что у вас все эти пакеты, условно, глобально стоят. Но Никс просто подобрат такое окружение, где они все есть, а ничего лишнего нет. Поэтому для разработки там всё равно нужен cabal-install или stack. Только Никс помогает создать вот эти вот окружения.

РЧ: Эти окружения, они переносятся и на cabal-install. То есть cabal-install видит какую-то пакетную датабазу в которой только нужные пакеты стоят?

ДР: Именно. С точки зрения Кабала это, как бы, глобальная база с пакетами и ты, когда загружаешься в этот Шелл у тебя Кабал их видит. А если там есть всё необходимое, то соответственно ты проект можешь собрать. В любом случае мне кажется, что дорожка эта интересная, правда, пока не проторенная. С интеграцией Стека, Никса и так далее. И, возможно, тут есть какие-то очень хорошие комбинации, которые просто ещё не нащупаны. Посмотрим, как оно пойдёт.


РЧ: В GHC завели новый баг о том, что GHC долго компилирует пакеты. Я думаю любой, кто пытался скомпилировать haskell-src-ext знает, что это такое - долгая компиляция. И на эту тему есть отдельный баг в GHC, который я когда-то зафайлил, но этот баг, о котором я сейчас говорю, [под номером 9630](https://ghc.haskell.org/trac/ghc/ticket/9630)... О, кстати, как интересно, арифметическая прогрессия получается, 9-6-3-0, красиво. Так вот, этот баг, я так понимаю, он возник из-за того, что медленно стала собираться библиотека Cabal, которая зависит от библиотеки binary, соответственно медленно стала собираться библиотека binary в первую очередь. Причём эта регрессия, я так понимаю, появилась именно в GHC HEAD, в том, что станет 8.0. Стали разбираться. В общем-то, как всегда, проблема с дерайвингом. Я, когда разбирался с haskell-src-ext, там такая же самая проблема была. Если вы будете дерайвить меньше инстансов, код будет компилироваться гораздо быстрее. То есть время занимает генерация этого кода, дерайвинг и затем оптимизация его. И действительно, тут в тикете есть профайл из которого видно, что больше всего времени занимает фаза SimplTopBinds. То есть, очевидно, это какая-то оптимизация top level bindings. Какие-то промежуточные результаты есть, баг всё ещё не пофикшен. Мы надеемся, что его пофиксят в самом GHC, но, по крайней мере, есть workaround, который Эдвард Янг предложил в библиотеку binary и workaround достаточно забавный. Когда мы делаем дерайвинг с помощью generic, то, оказывается... Ну, представьте себе, что мы используем стандартный GHC Generics, чтобы что-нибудь задерайвить и для этого нам надо определить дополнительные классы почти всегда. Потому что GHC Generics он для конструкторов типа, то есть там `* -> *`. Если вы хотите сделать это для обычного типа у которого kind `*`, то вам надо добавать этот параметр типа, который вы будете всегда игнорировать. Соотвественно есть класс Binary, есть класс GBinary, который почти как Binary, только кайнд `* -> *`. Так вот, Эдвард Янг нашёл, что гораздо лучше [разбить его на два](https://github.com/kolmodin/binary/commit/c641061fcf7886968218d25b7141bd7e0b60303a) поскольку в классе Binary 2 метода Put и Get, то оказывается, если вы сделаете 2 класса GBinaryPut и GBinaryGet, то это будет компилироваться намного быстрее. То есть там разница между 7 секундами и 1 секундой, что-то такое. Почему так происходит? Потому что по-разному представляются словари. Видимо, когда у вас один метод, то словарь, на него отдельный дайтатайп не заводится или там какой-то newtype, а если у вас несколько методов, то их надо паковать в record. И по какой-то причине, видимо, оптимизатор GHC гораздо хуже справляется с этими рекордами. Поэтому в качестве временного воркараунда, если у вас есть подобный код, где вы используете Generics и который долго компилируется, попробуйте разбить класс на как бы подклассы у которых только один метод. Но я надеюсь, что всё-таки этот баг пофиксят и нам не придётся так делать.

ДР: Это актуально только для 7.8.3?

РЧ: Если я правильно понимаю, это даже не для 7.8 актуально, это для 8.0. То есть в текущей версии HEAD эта регрессия появилась. В 7.8, вроде бы её нет.

ДР: В тикете написано 'significant performance regression with respect to GHC 7.8.3'.

РЧ: По сравнению с 7.8.3 у нас регрессия. То есть в 7.8.3 было всё хорошо, по сравнению с 7.8.3 ухудшилось.

ДР: Окей. Если я правильно тебя понял, предлагают сейчас проблему отсрочить путём генерирования меньшего количества инстансов.

РЧ: Как раз количество инстансов такое же или даже чуть больше, просто GHC, очевидно, лучше справляется с классами у которых один метод.

ДР: Ага, понятно.

РЧ: Но опять таки, мы надеемся, что всё-таки это пофиксят в GHC. И там есть несколько таких багов, которые говорят о том, что GHC постепенно становится медленнее и медленнее и хочется верить, что разработчики до них доберутся. Я на часть из этих багов подписан, часть из них я сам репортил и мне иногда приходят имейлы, что там какая-то активность есть. Какой-нибудь тэг, например, добавили. Ну, уже приятно, значит кто-то  на него смотрит, кто-то о нём помнит.


ДР: Я на этой неделе почитал [статью](http://www.leonmergen.com/haskell/crypto/2015/03/21/on-the-state-of-cryptography-in-haskell.html). Автора зовут Леон Мерген. Он пишет про текущее состояние криптографических библиотек в Хаскеле. Я, конечно, в криптографии ничего не понимаю и мимо проходил, но количество драмы в этом посте оно такое высокое, что я рекомендую её почитать. Автор рассказывает о том, как он знакомится и знакомился с криптопакетами всякими, которые на Hackage есть. Он начинает с того, как вообще люди энтропию генерируют. Он говорит, что есть некоторое количество методов, где-то генерируется энтропия из юзерского ввода, из положения курсора и так далее. И в какой-то момент появилась открытая спецификация для инструкции процессорной, которая называется `rdrand` и процессоры от Интел, современные, они имплементят эту инструкцию, но про имплементацию мы ничего толком не знаем. В постсноуденское, как он пишет, время нужно очень внимательно к этому подходить. И мне понравилось, как он пишет про это - tinfoil-hattedness надо. Нужно про это думать, как будто шапочку из фольги хотите. Он поминает такую драму из мейллистов, посвящённых ядру Линукс. Там как раз это давление со стороны Интела, оно вынудило ментейнера `/dev/random` вообще уйти. То есть одни говорили, что нужно переходить на эту инструкцию, потому что она быстрее, а с другой стороны говорили, что мы не можем энтропию бросить на откуп вашей закрытой имплементации. Это всё предыстория. Автор стал разбираться, как же в Хаскеле это всё используется. Он описывает, что есть несколько пакетов, на которые депендятся практически все остальные пакеты. Один из них называется [entropy](https://hackage.haskell.org/package/entropy) и он делает следующее. Если он видит, что вы можете использовать rdrand, он использует rdrand всегда. И это повышает уровень контроверси в комьюнити, потому что есть люди, такие как автор этого пакета, которые говорят "всё норм", а есть люди вроде ментейнеров Дебиана, которые новую версию entropy даже не пропустили, когда поддержку rdrand туда добавили. Потом автор смотрит на пакет [crypto-random](https://hackage.haskell.org/package/crypto-random) и там точно такая же ситуация. Дальше он пишет: "Я не думал, что когда-нибудь это скажу, но я на самом деле рекомендую вам использовать OpenSSL, если вам нужно генерить что-то рандомное в Хаскеле. Он предоставляет на самый удобный интерфейс, но вы, по крайней мере, будете знать, что вы получите." Статья очень классная и я её рекомендую, даже если это не ваше увлечение, как в случае со мной, но её просто интересно очень захватывающе и интересно читать.

(28:22)
