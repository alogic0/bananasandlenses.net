http://bananasandlenses.net/episode018

РЧ: Доброго времени суток, вы слушаете 18 выпуск подкаста бананы и линзы. С вами сегодя Денис Редозубов

ДР: Привет

РЧ: Денис Шевченко

ДШ: Приветствую

РЧ: и я, Роман Чепляка. Первая тема у нас сегодня посвящена расширению Strict, которое мы обсуждали в предыдущих выпусках. Это расширение, которое позволяет анотировать модуль и код в этом модуле будет компилироваться как строгий код. Это, по крайней мере, идея. Были спекуляции на тему того, что это расширение сделает Хаскель более простым для новичков, которым не хочется разбираться в тонкостях ленивых вычислений. Однако, как выяснилось, чтобы разбираться, что же делает эта прагма Strict, даже не все авторы компилятора это понимают. И вот, на этой неделе, было письмо в рассылку ghc-devs от Омера Синана Агасана, как-то так, который интересуется, а почему ... Значит, он написал функцию zip, которая деконструирует списочные аргументы на head и tail и вот эти head и tail, несмотря на то, что модуль анотирован, как Strict, они строго не вычисляются. То есть `zip (h:hs) (y:ys)` и вот эти `x` и `y` продолжают быть санками(thunk). Он спрашивает: "как же так и почему?". И получилась довольно интересная дискуссия, в которой было несколько версий, как же всё-таки должен работать Strict. То есть в первом приближении всё понятно и все были согласны, что Strict должен всё форсировать, но когда стали углубляться в детали, стало понятно, что ничего не понятно. Я предложил своё видение этого, которое в чём-то основано на другом, похожем расширении - это [UnliftedDataTypes](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes). Я напомню, UnliftedDataTypes делает некоторые типы unlifted и это означает, что все байндинги этого типа автоматически вычисляются. То есть мы работаем в терминах именно байндингов, в терминах переменных, и мне казалось эта концепция хорошо переносится сюда. Вот я предложил ... Тут что важно, важно, может быть, даже не то, какая именно будет семантика, важно чтобы она была простой. Чтобы можно было любому человеку объяснить в двух словах что делает прагма Strict. И моя попытка объяснить была именно такой, что если у нас есть модуль Strict, то все переменные, которые там определены, будут автоматически вычислены. Ну и, наверное, стоило бы добавить, что и function applications тоже будут автоматически вычислены. Хотя, может быть, это и следует. Да, главная идея такая, что любое определение будет строго вычислено. С другой стороны, Саймон Пейтон Джонс предложил своё определение. Определение Саймона тоже на первый взгляд простое и понятное. Саймон говорит, что код, который скомпилирован со Strict, не будет создавать санков. То есть он определяет это не в терминах такой денотационной семантики и не в терминах синтаксических, а в терминах операционных. Код, как он будет работать? Он будет работать каким-то таким образом, чтобы не создавать санков. Но там есть проблема с определением Саймона. Я, по крайней мере, 2 проблемы нашёл и Саймон со мной согласился и нашёл ещё третью проблему в собственном определении.

Во-первых, что будет, если у нас какой-то байндинг не используется? Тут ещё надо сказать, вот мы использовали аналогию с UnliftedDataTypes, ещё есть аналогия с [Bang Patterns](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/bang-patterns.html). То есть интуитивно в модуле Strict у нас все patterns с восклицательными знаками. И что будет, если мы, например, скажем, что `let _ = чему-то-undefined in что-то`. Вот будет это `undefined` вычислено или нет? По аналогии с `bang patterns` он будет вычислен даже несмотря на то, что он не используется. Но, по определению Саймона, полностью легально его просто выкинуть и не вычислять, потому что коль скоро мы не создаём санков, то всё хорошо. Но понятно, что это не очень согласуется с идеей строгого вычисления всего и, естественно, семантика у этих двух вычислений очень разная, потому что в одном случае мы получаем исключение, в другом случае мы исключения не получаем.

Другой пример, который я привёл, это `f g = g undefined`. То есть мы вызываем какую-то функцию с аргументом `undefined`. По логике, коль скоро наш модуль Strict, то эта аппликация должна быть строгой, то есть должно получиться исключение. Даже если `g` это какой-нибудь `const`, который аргумент свой не вычисляет, но коль скором мы в строгом модуле передаём в функцию `undefined`, то должно получиться исключение. Правда же. То есть это применение должно быть строгим. Но, поскольку `undefined` это не санк, это некий CAF([constant applicative form](https://wiki.haskell.org/Constant_applicative_form)), который определён... Вернее, это санк, но он определён в другом модуле. То есть, может быть, изначально он что-то типа санка, но потом он заменён исключением и каждый раз, когда мы обращаемся к `undefined` мы получаем то самое исключение. Как-то так. Ну суть важно. Главное, что `undefined` для наших целей - это указатель. То есть это на санк, который мы создаём, идея Саймона была в том, что мы не создаём новых санков, но это уже CAF, который уже создан. Мы же его не создаём, мы его просто передаём и поэтому тоже легально его не вычислять в этом контексте. Потом Саймон сам задаётся вопросом - а действительно, что же будет, например, если мы просто вызываем что-то типа `f (g x)`, где `f` определена в ленивом модуле? То есть будет ли эта аппликация ленивой или строгой? То есть с одной стороны она должна быть ленивой, поскольку `f` определена в ленивом модуле, с другой стороны, она должна быть строгой, потому что `f (g x)` определена в строгом модуле. В общем ничего не понятно с этой прагмой.

ДШ: Сами себе проблемы сделали, теперь гадают, как же мы это разрулим.

РЧ: Да, да. Типа того. Вот авторы этой прагмы, это Йохан Тибль и Адам Сендберг Эриксон. И я так понимаю, что это был Google Summer of Code проект Адама и вот они тоже, как бы, над этим думают. Йохан отписался, что у него мало времени, чтобы над всем этим думать. И сейчас, поскольку это уже должно войти в GHC 8.0, который вот-вот, совсем скоро, должен быть выпущен, то консенсуc такой, что давайте мы попробуем задокументировать как-то что же мы тут наделали и как оно должно себя вести, а дальше будем думать.

ДР: У меня не до конца срастается ещё, как эта прагма должна работать с какими-нибудь бесконечными структурами, например?

РЧ: Ну, там проще, потому что мы только говорим, что это Weak Head Normal Form. То есть оно не полностью вычисляется, оно вычисляется только до WHNF. Вопрос в том, что именно будет вычисляться до WHNF? Потому что, если, например, у нас есть сложные вложенные patterns, то будет ли только верхний уровень вычислен до WHNF или будут все отдельные переменные вычислены до WHNF? 

ДР: А когда ты работаешь с бесконечным списком, как про это думать надо?

РЧ: Когда ты работаешь с бесконечным списком, ты не можешь его заматчить бесконечно.

ДР: Но если ты ожидаешь, что код будет строго исполняться, если мы говорим, что можем рассуждать про это, как про операционную семантику, не про денотационную, то как-то этот кейс не клеится. Особенно в рамках того, что ты говоришь, что все хотят иметь объяснение, когда можно на двух пальцах объяснить человеку, который не знаком с этой прагмой, что она вообще делает. Тут такие кейсы возникают. Ты только назвал 3 штуки и всё равно возникает куча вопросов.

РЧ: Кстати да, насчёт бесконечных списков, мне даже сложно сказать, как оно должно быть. Но, возможно, у тебя даже не получится сконструировать бесконечный список в строгом модуле. Другое дело, что ты всегда можешь импортировать его из ленивого модуля.

ДР: Ну да. А тут другой вопрос поднимается. Если мы хотим утвержадать, что в этом модуле всё Strict, мы, это спекуляция, конечно, но, возможно, мы не можем депендится на функции, которые определены в нестрогих модулях.

РЧ: Не-не, однозначно можем. Это одно из фундаментальных требований. Иначе всё вообще идёт к чертям, потому что Base выбрасываем и так далее.

ДР: Да, про это было бы очень просто рассуждать, если бы там был какой-то контроль изоляции, можно было бы сказать, что вот это мы юзаем только с Base или только с чем-то, есть минимальные какие-то определения, можете использовать их, импортить в других модулях, но не наоборот. Просто это было бы более понятно с точки зрения какого-то опыта работы...

РЧ: Ну, оно было бы более понятно, но менее полезно. Тут, всё-таки, акцент на прагматичность.

ДР: Полезность, да.

ДШ: Честно говоря, смотрю со стороны и получилось так - сделали идею, которая породила больше потенциальных проблем и обсуждений, нежели принесла какую-либо пользу. Вам не кажется?

РЧ: Она не то чтобы породила. Тебя же никто не заставляет эту прагму всключать и коль скоро ты её не включаешь, у тебя никаких проблем нет. Поэтому это не тот случай, когда из-за того, что не добавили фичу, что-то другое сломалось. Просто вопрос в том, как сама эта фича должна работать. В GHC 8.0 она экспериментальная будет. Если выяснится, что она должна работать как-то по-другому или даже, что это вообще была плохая идея, давайте её выкинем, то, в общем, никто не мешает её взять и выкинуть.

ДР: Ну и Йохан в общем-то открытым текстом писал, что это интересно, как такая ресёрч фича. То есть мы не знаем, как это использовать, но мы бы хотели найти способ.

Так как у нас подкаст про Stack, то я немного про Стек расскажу. Вот обновил я Стек и там появилась [поддержка Nix](http://docs.haskellstack.org/en/stable/nix_integration.html). Я даже пробовал это использовать и сейчас хочу вам рассказать, как я думал это использовать и, может быть, кто-то из вас подкинет мне идею, как это можно развивать и куда это должно в конце-концов прийти.

ДШ: Да, да. Это весьма интересно, расскажи, пожалуйста!

ДР: Смотри, у нас есть Стек, у нас есть Никс. Никс позволяет нам делать строго больше, чем Стек. Он позволяет нам депендиться на какие-то вещи за пределами хаскельной инфраструктуры, а Стек позволяет нам только менеджить библиотеки, проекты из нескольких cabal-проектов и версию компилятора. Если включить в Стеке поддержку Никса, вы можете добавлять в реквайрементсы названия Никс-пакетов или как оно там называется, деревейшн. Я буду говорить пакетов. Вы можете описывать такие ситуации, что для сборки вот этого проекта на Хаскеле Стеком, мне нужен ещё Перл. Ну, потому что вы в тестах, там, внезапно вызываете Перл для того, чтобы использовать, ну я не знаю, странная у вас конструкция такая.

РЧ: Да, на самом деле Перл в хаскельных проектах незаменим. И я его пару раз использовал, как дешёвую замену Темплейт Хаскелю. С Темплейт Хаскелем надо каждый раз идти, читать документацию, какие там конструкторы, как их применять, потом его немножко отлаживать. А тут, когда кучу кода надо нагенерировать, я пишу за полминуты какой-нибудь перловый скрипт, который генерирует 2 экрана кода, запихиваю его в git и всё круто.

ДР: А у нас другая история была. У нас, например, тесты депендятся на Ruby, потому что библиотека для работы с последним драфтом JSON Schema, она называется [hjsonschema](https://github.com/seagreen/hjsonschema), она нормально скопы не умеет рулить. И я залез туда внутрь, начал писать патчик и что-то быстро не осилил. Потом вопрос снова поднялся, залез коллега, посмотрел внутрь и говорит: "быстро не сделать". Мы взяли воткнули в тесты просто вызов рубишный, который готовыми библиотеками валидирует ту же джейсон-схему. Так у нас появился депенденси в тестах.

РЧ: Я только одного не понимаю. Ты рекламируешь Стек, как решение для внешних зависимостей? Мне казалось, что Никс наоборот, гораздо больше для этого подходит.

ДР: Да, я не дорассказал, я только проблематику сейчас озвучил. В Никсе пявился флажочек новый и опции, которые можно в конфиге указать. Флажочек `--nix` и он включает поддержку Никс и пытается поставить все пэккеджи, которые у вас там указаны. Таким образом, если у вас тесты внезапно, в каком-то месте зовут перлы или Руби, вы сможете эту ситуацию описать и, если на машине, где вы делаете `stack build` или `stack test` у вас настроен Nix, то Стек автоматически из Никса подтянет все эти зависимости и сделает hopefully правильную вещь. Но это минимальный кейс и на его основе хотелось бы построить что-нибудь покруче. Потому что в Никсе есть поддержка хаскельных пакетов и она уже давно, довольно взрослая. По крайней мере, так позиционируется.

ДШ: Более того, Денис, прошу прощения, я добавлю, что в Никсе не только есть поддержка хаскельной инфраструктуры, скажем так, а там ещё и поддежка [LTS](http://www.stackage.org/lts).

ДР: Вот, я к этому как раз веду, потому что в Никсе вы можете найти любой LTS Stackage и таким образом вы может на халяву получить бинарный кэш. То есть, если вы используете Stackage, вам нужно скачать библиотеки и потом их компилять упорно и до победы. А если вы используете этот же снэпшот LTS из Никса, вам достаточно будет скачать просто готовые эти пакетики. Естественно тут проблема, где как раз уже нужен Стек, это когда вы не готовы свой проект взять и резко перевести на Никс. Потому что у вас есть какие-то зависимости, которых нет в Никс-пэккеджес, ну, версии библиотек, например. Или вы депендетесь на какие-то форки библиотек, вам нужно, например, с Гитхаба что-то взять.

РЧ: А разве Никс это не может сделать? Ты же можешь сделать свой собственный пакет, добавить в Никс.

ДР: Да, ты можешь это сделать, но для того, чтобы это сделать, тебе нужно с Никсом разобраться. А я говорю про ситуацию, когда ты можешь плавно себе что-то улучшить. По крайне мере, что я думаю сделать. Я думаю максимум зависимостей перевести на LTS, чего, например, у меня на работе не сделано. Это просто сократит время компиляции и позволит бинарный кэш из Никса использовать. А всё остальное, пока что, на следующем этапе, видимо, будет Стеком тянуться. Просто потому, что Никс он своеобразен, он требует времени для того, чтобы всё это ввернуть. И насколько я знаю, там нет способа это лучше сделать, кроме как мейнтейнить собственный форк Никс-пэккеджес, что может быть проблемно и времезатратно, если вы хотите это всё время поддерживать.

РЧ: Знаешь, Денис, что мне это напоминает? Напоминает ту же логику, что я хочу писать на Хаскеле, но поскольку весь Хаскель это сильно сложно, то я включу себе `-XStrict` и постепенно буду Хаскель изучать. Сначала без ленивости, а потом и ленивость. Вот примерно то же ты говоришь, потому что, я думаю, если ты будешь пытаться использовать Никс и Стек вместе, то ты огребёшь гораздо больше проблем и тебе придётся гораздо глубже копаться в Никсе, чтобы их пофиксить, чем если бы ты просто использовал Никс. Потому что Никс - это уже некая проторенная дорожка. Много там всяких постов, есть там какой-нибудь IRC канал, наверное, где ты можешь спросить "почему у меня это не работает". А тут ты будешь постоянно огребать какие-то непонятные проблемы, с которым ты идёшь к никсовым разработчикам, а они говорят "это ваш непонятный Стек чего-то не то делает". Ты идёшь в Стек, а они говорят "окей, мы пофиксим это в следующем релизе" и так далее.

ДР: Но тут есть момент, про который я не рассказал. В Никсе вы можете описать, как ваш пакет нужно собирать на собственном никсовом языке. В Стеке вы можете указать вот этот файлик, где вы всё описали. Поэтому я думаю, что можно использовать схему, где как раз очень-очень тонкая прослойка Стека это использует. Причём настолько тонкая, что вы Стек, вероятно, можете после этого одним дуновением ветра выбросить. Поэтому здесь есть, воможно, какой-то middleground, если вы хотите в эту сторону двигаться. Я пока стабильную почву под ногами не нашёл на эту тему, но хотелось бы попробовать в будущем.

РЧ: Из того, что ты рассказал, мне представляется, что эта фича не для тех, кто использует Никс в первую очередь, а для тех, кто использует Стек. Если вы используете Стек и вам надо что-то левое поставить, то можете, допустим, в добавок, принести себе Никс и использовать его, как помощник Стеку. Но, если вы в такой ситуации, как, например, Шевченко у нас, у которого всё хозяйство построено на Никс, то нести туда Стек, наверное, не очень осмысленно.

ДШ: Вообще бессмысленно.

РЧ: С другой стороны, я же слышал все эти жалобы от пользователей Никс, что когда ты просто компилируешь пакеты, то всё окей. Но когда ты должен их разрабатывать и, там, тебе нужен тот же GHCi, тебе надо что-то там постоянно перекомпилировать, то, вроде как, Никс для этого не очень удобен и они всё равно используют cabal для которого Стек является лучшей альтернативой.

ДР: Подожди. Кабал ты не можешь исключить из уравнения вообще.

РЧ: Я имею в виду cabal-install, как программу.

ДР: Тебе не нужно его исключать, Никс всё равно его использует, потому что Никсу чем-то нужно собирать хаскельные проекты. А для разработки там используется щтука под названием Nix-shell. Она представляет собой следующее - вы описываете что вы хотите видеть на языке Никс в своём окружении и потом говорите, там... То есть, например, из манифеста Кабала, который точка-кабал, можно сгенерировать автоматически утилитой cabal2nix файл с расширением nix. Который будет все пакеты просто перечислять и попытается их поставить Никсом. И вы можете shell открыть в этом окружении. Идея в том, что вы попадаете в окружение, где у вас есть только всё нужное и нет ничего ненужного. И если в этом окружении сказать `cabal build` оно должно работать, потому что у вас все эти пакеты, условно, глобально стоят. Но Никс просто подобрат такое окружение, где они все есть, а ничего лишнего нет. Поэтому для разработки там всё равно нужен cabal-install или stack. Только Никс помогает создать вот эти вот окружения.

РЧ: Эти окружения, они переносятся и на cabal-install. То есть cabal-install видит какую-то пакетную датабазу в которой только нужные пакеты стоят?

ДР: Именно. С точки зрения Кабала это, как бы, глобальная база с пакетами и ты, когда загружаешься в этот Шелл у тебя Кабал их видит. А если там есть всё необходимое, то соответственно ты проект можешь собрать. В любом случае мне кажется, что дорожка эта интересная, правда, пока не проторенная. С интеграцией Стека, Никса и так далее. И, возможно, тут есть какие-то очень хорошие комбинации, которые просто ещё не нащупаны. Посмотрим, как оно пойдёт.


РЧ: В GHC завели новый баг о том, что GHC долго компилирует пакеты. Я думаю любой, кто пытался скомпилировать haskell-src-ext знает, что это такое - долгая компиляция. И на эту тему есть отдельный баг в GHC, который я когда-то зафайлил, но этот баг, о котором я сейчас говорю, [под номером 9630](https://ghc.haskell.org/trac/ghc/ticket/9630)... О, кстати, как интересно, арифметическая прогрессия получается, 9-6-3-0, красиво. Так вот, этот баг, я так понимаю, он возник из-за того, что медленно стала собираться библиотека Cabal, которая зависит от библиотеки binary, соответственно медленно стала собираться библиотека binary в первую очередь. Причём эта регрессия, я так понимаю, появилась именно в GHC HEAD, в том, что станет 8.0. Стали разбираться. В общем-то, как всегда, проблема с дерайвингом. Я, когда разбирался с haskell-src-ext, там такая же самая проблема была. Если вы будете дерайвить меньше инстансов, код будет компилироваться гораздо быстрее. То есть время занимает генерация этого кода, дерайвинг и затем оптимизация его. И действительно, тут в тикете есть профайл из которого видно, что больше всего времени занимает фаза SimplTopBinds. То есть, очевидно, это какая-то оптимизация top level bindings. Какие-то промежуточные результаты есть, баг всё ещё не пофикшен. Мы надеемся, что его пофиксят в самом GHC, но, по крайней мере, есть workaround, который Эдвард Янг предложил в библиотеку binary и workaround достаточно забавный. Когда мы делаем дерайвинг с помощью generic, то, оказывается... Ну, представьте себе, что мы используем стандартный GHC Generics, чтобы что-нибудь задерайвить и для этого нам надо определить дополнительные классы почти всегда. Потому что GHC Generics он для конструкторов типа, то есть там `* -> *`. Если вы хотите сделать это для обычного типа у которого kind `*`, то вам надо добавать этот параметр типа, который вы будете всегда игнорировать. Соотвественно есть класс Binary, есть класс GBinary, который почти как Binary, только кайнд `* -> *`. Так вот, Эдвард Янг нашёл, что гораздо лучше [разбить его на два](https://github.com/kolmodin/binary/commit/c641061fcf7886968218d25b7141bd7e0b60303a) поскольку в классе Binary 2 метода Put и Get, то оказывается, если вы сделаете 2 класса GBinaryPut и GBinaryGet, то это будет компилироваться намного быстрее. То есть там разница между 7 секундами и 1 секундой, что-то такое. Почему так происходит? Потому что по-разному представляются словари. Видимо, когда у вас один метод, то словарь, на него отдельный дайтатайп не заводится или там какой-то newtype, а если у вас несколько методов, то их надо паковать в record. И по какой-то причине, видимо, оптимизатор GHC гораздо хуже справляется с этими рекордами. Поэтому в качестве временного воркараунда, если у вас есть подобный код, где вы используете Generics и который долго компилируется, попробуйте разбить класс на как бы подклассы у которых только один метод. Но я надеюсь, что всё-таки этот баг пофиксят и нам не придётся так делать.

ДР: Это актуально только для 7.8.3?

РЧ: Если я правильно понимаю, это даже не для 7.8 актуально, это для 8.0. То есть в текущей версии HEAD эта регрессия появилась. В 7.8, вроде бы её нет.

ДР: В тикете написано 'significant performance regression with respect to GHC 7.8.3'.

РЧ: По сравнению с 7.8.3 у нас регрессия. То есть в 7.8.3 было всё хорошо, по сравнению с 7.8.3 ухудшилось.

ДР: Окей. Если я правильно тебя понял, предлагают сейчас проблему отсрочить путём генерирования меньшего количества инстансов.

РЧ: Как раз количество инстансов такое же или даже чуть больше, просто GHC, очевидно, лучше справляется с классами у которых один метод.

ДР: Ага, понятно.

РЧ: Но опять таки, мы надеемся, что всё-таки это пофиксят в GHC. И там есть несколько таких багов, которые говорят о том, что GHC постепенно становится медленнее и медленнее и хочется верить, что разработчики до них доберутся. Я на часть из этих багов подписан, часть из них я сам репортил и мне иногда приходят имейлы, что там какая-то активность есть. Какой-нибудь тэг, например, добавили. Ну, уже приятно, значит кто-то  на него смотрит, кто-то о нём помнит.


ДР: Я на этой неделе почитал [статью](http://www.leonmergen.com/haskell/crypto/2015/03/21/on-the-state-of-cryptography-in-haskell.html). Автора зовут Леон Мерген. Он пишет про текущее состояние криптографических библиотек в Хаскеле. Я, конечно, в криптографии ничего не понимаю и мимо проходил, но количество драмы в этом посте оно такое высокое, что я рекомендую её почитать. Автор рассказывает о том, как он знакомится и знакомился с криптопакетами всякими, которые на Hackage есть. Он начинает с того, как вообще люди энтропию генерируют. Он говорит, что есть некоторое количество методов, где-то генерируется энтропия из юзерского ввода, из положения курсора и так далее. И в какой-то момент появилась открытая спецификация для инструкции процессорной, которая называется `rdrand` и процессоры от Интел, современные, они имплементят эту инструкцию, но про имплементацию мы ничего толком не знаем. В постсноуденское, как он пишет, время нужно очень внимательно к этому подходить. И мне понравилось, как он пишет про это - tinfoil-hattedness надо. Нужно про это думать, как будто шапочку из фольги хотите. Он поминает такую драму из мейллистов, посвящённых ядру Линукс. Там как раз это давление со стороны Интела, оно вынудило ментейнера `/dev/random` вообще уйти. То есть одни говорили, что нужно переходить на эту инструкцию, потому что она быстрее, а с другой стороны говорили, что мы не можем энтропию бросить на откуп вашей закрытой имплементации. Это всё предыстория. Автор стал разбираться, как же в Хаскеле это всё используется. Он описывает, что есть несколько пакетов, на которые депендятся практически все остальные пакеты. Один из них называется [entropy](https://hackage.haskell.org/package/entropy) и он делает следующее. Если он видит, что вы можете использовать rdrand, он использует rdrand всегда. И это повышает уровень контроверси в комьюнити, потому что есть люди, такие как автор этого пакета, которые говорят "всё норм", а есть люди вроде ментейнеров Дебиана, которые новую версию entropy даже не пропустили, когда поддержку rdrand туда добавили. Потом автор смотрит на пакет [crypto-random](https://hackage.haskell.org/package/crypto-random) и там точно такая же ситуация. Дальше он пишет: "Я не думал, что когда-нибудь это скажу, но я на самом деле рекомендую вам использовать OpenSSL, если вам нужно генерить что-то рандомное в Хаскеле. Он предоставляет на самый удобный интерфейс, но вы, по крайней мере, будете знать, что вы получите." Статья очень классная и я её рекомендую, даже если это не ваше увлечение, как в случае со мной, но её просто очень захватывающе и интересно читать. Собственно ликбез под этим - если вам это важно, если у вас есть какие-то чувствительные данные и чувствительный кусок кода, где вам нужно генерировать действительно случайные числа и распределение должно быть правильным, имейте в виду и, возможно, используйте OpenSSL. Кстати, ещё один момент очень инересный. Он, повышая градус паранойи, пишет про пакет entropy и говорит: "Обратите внимание, что разработчиком пакета выступает компания Галуа, которая известна своими разработками для американской военки". Типа, вспомните бэкдоры в железках, вспомните Сноудена и так далее. Очень драматично. И ещё одна рекомендация, я чуть не упустил. Есть одна библиотека, которая сейчас находится в разработке, она называется libsodium. Она призвана как раз все эти проблемы устранить. И на Хаскеле уже есть обвязка для неё, которая называется [saltine](https://hackage.haskell.org/package/saltine). Там открытым текстом написано, что оно не production ready, но, судя по этому обзору, оно всё находится примерно на одном уровне. Так что либо OpenSSL, либо мы ждём новый saltine.

РЧ: С Солтин забавная история. Если я не ошибаюсь, Солтин написал мой бывший коллега, Джо Абрамсон. Он её написал для версии софта, которую мы потом склонировали. У нашей компании была материнская компания и вот этот Солтин писался для неё. Они там работали с медицинскими записями. Так вот я даже не знаю, честно говоря, до сих пор она используется у них или нет. По крайней мере я когда пришёл и мы форкнули их кодбазу, я очень быстро от неё избавился. Потому что собирать её было ещё такое удовольствие. Там надо было с какого-то одного гитхаба установить Sodium, потом с другого гитхаба поставить этот Saltine...

ДР: А вот был бы у тебя Стек с Никсом, ты бы сразу всё взял, поставил.

РЧ: О, да. Касательно статьи самой... Вот ты сказал, что ты не специалист в криптографии. Я, естественно, не специалист в криптографии и в таких случаях очень выжно выслушать обе стороны. Потому что приходит какой-нибудь чувак и гонит. И, поскольку мы не специалисты, мы такие "да-да". Ну, он звучит очень умно и, наверное, он прав. Статья, на самом деле, довольно старая, она ещё от марта и на Реддите есть [обсуждение](https://www.reddit.com/r/haskell/comments/2zsbth/on_the_state_of_cryptography_in_haskell/) и, в частности, там даёт комментарий Том Дюбуасон, который один из авторов библиотек, которые обсуждаются в статье. И он довольно адекватно отвечает на большую часть из претензий. Я сейчас не буду защищать ни одну ни другую сторону просто потому, что я в этом мало понимаю, но обязательно, когда вы такие статьи читаете, постарайтесь найти ответные. В shownotes я добавлю и статью и обсуждение, обязательно почитайте и то и другое.

ДР: Ещё один маленький вопросик я хотел с вами обсудить. Буквально вчера посмотрел библиотечку [cats](https://github.com/non/cats) для Скалы. Для тех, кто не в курсе, это такой foundation package для того, чтобы всякие абстрактные примитивы реализовывать на Скале. Категориальные, так сказать, структуры, всем вам, хаскелистам, знакомые. Они там используют какую-то библиотеку, которая даёт им сахар для того, чтобы описывать там тайп-классы на Скале у которых, наверное, проблемы с когерентностью, но не суть. И они описывают там такие вещи, как функторы, монады и так далее. Что мне показалось очень интересным, это то, что они вместе с классами для этих всех вещей, они ещё и предоставляют проперти для их тестирования. И у меня возник вопрос - а почему мы это в Хаскеле не делаем? И правльно ли, что мы это не делаем? Потому что с одной стороны у всех этих вещей есть законы и если ваш инстанс не соответствует законам, то он неправильный и вы не можете ожидать от него корректной работы. Иметь какой-то способ тестировать законы, хотя бы эмпирически, было бы прикольно, как мне кажется. Таким образом, если библиотека вам предоставляет ещё какой-то проперти, который вы можете засунуть в свои тесты, в QuickCheck и протестить на каком-то семплированом наборе данных, то это было бы здорово. С другой стороны тут могут возразить, что мы же всё равно не можем быть в этом уверены на 100%, потому что у нас нет доказательства. Хаскель не позволяет нам такие пропозишены доказывать и, может быть, это было бы здоровым компромиссом между ничем, описанием просто этих законов в комментариях и в документации и хотя бы чем-то, хоть частичной поддержкой наших гипотез. Я имею в виду гипотезы - это когда мы пишем инстанс для конкретного класса. Как вы думаете?

РЧ: Я не знаю, почему ты решил, что мы этого не делаем. Во-превых, если я не ошибаюсь, на Hackage был, может быть даже не один, пакет, который определял QuickCheck свойство для стандартных классов. То есть, например, вот тебе квикчек проперти, которая проверяет, там, монадные законы. Ты просто подставь свой тип. Тебе же свой тест даже не надо писать, он всегда один и тот же. Там всегда три закона, которые надо проверить.

ДР: Да, Рома. Но вопрос, который я поднимаю, почему мы не предоставляем это с библиотеками, которые дают нам эти классы?

РЧ: Чтобы не зависеть от Квикчека, естественно. Ну с какого библиотека transformers будет зависеть от библиотеки quickcheck?

ДР: Да, я согласен, это резонно. Другой вопрос, наверное спорное утверждение, но мне кажется, что все используют Квикчек и то, что он находится в такой позиции, в которой сейчас находится, это только нам хуже делает. То есть, во-первых, мы могли бы его из коробки предоставить, а во-вторых, мы могли бы его немного осовременить. Потому что его в какой-то момент вместе с пейпером выкинули на Hackage и я помню, что для самого Квикчека монадные законы не выполнялись. Я помню, что нужно ставить дополнительные пакеты с заорфанеными инстансами для того, чтобы там с кучей других библиотек оно нормально работало, чтобы предоставить им Arbitrary. И Квикчек, он находится в такой нише, что он всем нужен, но всё сделано очень странно и выглядит криво.

РЧ: Так, давай по-порядку. Во-первых, Квикчек используют не все. Я имею в виду, что у Квикчека есть альтернативы. Есть [smallcheck](https://hackage.haskell.org/package/smallcheck), которого я являюсь мейнтейнером. И я большую часть использую smallcheck, изредка использую quickcheck. Есть [smartcheck](https://github.com/leepike/SmartCheck), который Ли Пайк пишет. Есть другие библиотеки. То есть quickcheck это далеко не единственный игрок.

ДР: Да, безусловно. Ты пойми правильно, я не говорю давайте все использовать Квикчек. Я говорю, что вообще хаскелисты любят property based тестинг.

РЧ: Любят или не любят?

ДР: Любят. И наличие нескольких библиотек для этого говорит о том, что любят. Я говорю, что хорошо было бы иметь это всё из коробки.

РЧ: Что из коробки?

ДР: Какой-то стандартный метод для тестирования проперти. Потому что это позволило бы делать такие вещи, как делают ребята в Кэтс, когда ты описываешь библиотеку и ты сразу предоставляешь там проперти для тестирования законов.

РЧ: Подожди, что значит сразу? Ну, come on... 

ДР: Давай последовательно разберёмся.

РЧ: То есть ты, например, устанавливаешь какую-то библиотеку, которая определяет какой-нибудь класс... Во-первых тут дело даже в том, что вот эти классы с законами, их на самом деле меньшинство.

ДР: Да, их очень мало, я согласен.

РЧ: То есть это классы из таких, типично Кметовских библиотек, которые там какой-то матан кодируют. Половина из этих классов вообще в Base. Особенно, которые люди на самом деле используют, зачастую в Base находятся. Это всякие там монады, функторы, аппликативные функторы. Дальше есть какие-то более маргинальные классы, которые Кмет определяет. Всякие там Bind, Apply, профункторы и вот у них ещё есть законы. И вот то, что опредеяет Кмет и что есть в Base, это, наверное, 90% таких классов у которых есть законы. Зачастую, когда определяется класс, у него законов либо вообще нет, либо они какие-то номинальные. Возьми, условно говоря, MySQLSimple. Там есть классы чего-то там FromField и ToField, или там FromRaw и ToRaw. И, в принципе,неплохо было бы , если бы они были обратные. И то, там есть звёздочка, потому что типы, скорее всего, не совсем сходятся. Или там FromJSON, ToJSON. Ну окей, они в одном порядке должны быть identity. Ради вот этого великого свойства ты хочешь добавлять зависимости от Квикчека? А кто-то хочет, опять таки, это Смолчеком тестировать. То есть, у большинства классов законов нет и надо научиться с этим жить. Есть люди, которые живут в каких-то песочных замках и считают, что вообще нельзя допускать классы без законов, но это, конечно, полная херня.

ДР: Но ты не сосредотачивайся полностью на том факте, что это законы для классов. Ты же можешь какие-то более комплексные свойства описывать. Ты же сам пример только что приводил. Ты говорил, что есть классы FromJSON, ToJSON и, если мы делаем `encode . decode`, то он должен быть `id`. И это имеет смысл и это не в рамках даже одного класса делается.

РЧ: Но это свойство ты пишешь даже не в строчку, а в треть, в четверть строчки. Поэтому никакого смысла не имеет импортировать это из библиотеки. Тем более, что это повлечёт за собой зависимость от какой-то другой библиотеки, типа Квикчека или Смолчека и повлечёт за собой ограничение, что ты будешь использовать это для Квикчека, а для Смолчека у тебя альтернативы нет.

ДР: Вот смотри почему я об этом говорю. Например, я использую подобные свойства на работе для того, чтобы тестировать, там, имплементацию каких-то своих парсеров, чего-то ещё. Даже вернее не парсеров, а каких-то сериализаторов, вот это такой чёткий пример получается. У тебя, допустим, есть какой-то API и ты в этот API в одном месте генерируешь простыню JSON, в другом месте ты парсишь это простыню. Ты ожидаешь получить, что у тебя то и другое будет соответствовать. И возникают всякие прикольные истории с тем, что, например, там в Aeson'е есть неправильные инстансы для работы с временем и таймзонами. Они вообще мало смысла имеют. Там нужны (неразаб.) таблицы, чтобы нормально это сделать. И то, как работает это из коробки, то, что там вообще даже представлен этот инстанс, оно даёт странное поведение, которое нужно потом сидеть, раскапывать. Моя позиция тут не в том, что давайте все возьмём строчно Квикчек и будем им пользоваться и законы кодировать. Я говорю, что мы могли бы быть более последовательны в том, что мы включаем в библиотеки. Потому что часть этих проблем они как раз выливаются в ваш код из библиотек с Хэкаджа. Недавно была у нас история как раз с тем же aeson. Потом были какие-то истории с Квикчек инстансами и так далее. Возможно я предлагаю какое-то радикальное решение. Может быть, это можно сделать как-то мягче, но проблема, на мой взгляд, есть. Она не супер-пупер острая, то есть это не прямо с бритвой у горла, но я испытываю недовольство от этого.

РЧ: То есть ты жалуешься на то, что aeson экспортирует какие-то инстансы, которые, там, не соблюдают законы. Да?

ДР: Ну, это один из примеров.

РЧ: Так для этого надо, чтобы тесты были в самом Аэсоне. Значит, просто, Аэсон либо плохо протестирован, либо, я не удивлюсь, если Брайен знает об этом и просто забил на эту проблему.

ДР: Там, скорее, и есть арканное знание какое-то. Но у тебя, как у пользователя, у тебя же нет способа это проверить. Ты не можешь зайти, открыть тесть и посмотреть, что вот это вот проперти, которое я знаю и я вижу откуда оно импортируется, оно тестируется. Но тут я понимаю, что много спорных моментов и утверждений, но мне кажется в каком-то виде это было бы очень клёво иметь.


РЧ: А что ещё было бы клёво иметь - это открытые кайнды и над этим уже началась работа. Есть GHC тикет, в котором обсуждают, как это сделать. В чём идея. У нас есть стандартный кайнд "звёздочка", в который попадают все типы, которые мы определяем в Хаскеле каждый раз, когда мы пишем `data T = чему-то-там`, то `T` у нас имеет кайнд `*`. С другой стороны есть расширение DataKinds, которое промоутит любой тип данных в кайнд, но этот кайнд закрытый. Если у вас у типа данных `T` есть конструкторы `C1`, `C2`, `C3`, то ему будет соответствовать кайнд `T` у которго будут типы `C1`, `C2`, `C3`, но это единственные конструкторы типов, которые населяют это кайнд. Сейчас что пытаются сделать? Пытаются сделать открытые кайнды. То есть, по аналогии со звёздочкой вы может определить свой кайнд, сказать, там, `kind K` и дальше в разных модулях, вот как вы можете определять инстансы для классов, так вы будете определять инстансы для кайндов. То есть, этот тип принадлежит этому кайнду. И, по-моему, это очень здорово.

ДР: Я не совсем понимаю импликейшнс этого. Можешь пример привести, когда это полезно?

РЧ: Ну окей, пример. Кстати, вполне возможно, что это где-то в Серванте будет полезно. Вы же там всё на уровне типов кодируете. Вот представь себе, что у тебя есть кайнд, какой-нибудь метод для HTTP. И у него стандартные, какие-нибудь инстансы GET, PUT, DELETE, POST, но ты ещё хочешь разрешить, чтобы твои пользователи могли тоже свои инстансы этого кайнда определять. То есть, с одной стороны, ты не хочешь это делать звёздочкой, потому что это слишком большой кайнд и не имеет смысла иметь HTTP метод Maybe Int, правда? Это имеет смысл только для некоторых типов, которые предназначены специально, как тэги для HTTP методов. Но в то же время он будет расширяемый. И такая проблема много где возникает, когда вы что-то кодируете на уровне типов. Наример, у нас в кодбазе используется UUID и чтобы сделать это более type safe, мы UUID тэгаем какими-то... То есть у нас там UUID Participant, UUID Programmer. Это фантомные типы, которые обозначают, к чему этот UUID относится, так что мы случайно не передадим UUID, который просто не имеет смысла в этом контексте.

ДР: Можно просто ньютайпы использовать для таких простых случаев, то.

РЧ: Нет, для этого используются не ньютайпы, для этого используются пустые дэйтатайпы.

ДР: А, ты хочешь в какой-то момент сказать, что вот эта функция должна работать с любым UUID, не важно чем от протэган?

РЧ: Э-э... Ну, это тоже бывает, но, в основном, идея такая, что определять newtype на каждый случай это совсем геморройно. Потому что придётся для каждого ньютайпа делать все инстансы. Ну, там, `instance Show`, `instance Pretty`, `instance Parse`...

ДР: Ну тогда `newtype ... deriving`. Вообще копипаста голимая, один раз написал и всё.

РЧ: Ну окей, эта копипаста, которая там три-четыре строчки, плюс ты не забывай, это, опять таки, размер кода. То есть это всё код, который будет генерироваться. Оно в итоге выльется в coercions, но всё равно это будут какие-то термы. То есть нет, это неправильный способ. К тому же есть функции, которые не принадлежат ...

ДР: Подожди, в какие термы оно выльется? Если ты newtype объявляешь, там нет таких проблем.

РЧ: Смотри, ты объявил newtype, ты сделал generalized newtype deriving, у тебя всё равно будет терм, который соответствует словарю этого инстанса. Потому что у каждого инстанса есть словарь. Другое дело, что этот словарь, он, скорее всего, будет напрямую обращаться к другому словарю. Я думаю, что код всё-таки будет какой-то сгенерирован минимальный.

ДР: Ну, идея-то как раз в ньютайпах и GND в том, что нет оверхеда в рантайме.

РЧ: А я не говорю про рантайм, я говорю про компайлтайм.

ДР: Компайлтайм, может быть.

РЧ: То есть это, какой-то дополнительный код, который GHC в компайлтайме должен ворочать и то, что мы уже сегодня обсуждали, что компиляция от этого может замедляться. Это раз. Во-вторых, не весь код это инстансы. Есть просто функции, которые... ну вот ты не хочешь по какой-то причине помещать в класс. Оно так гораздо проще и гораздо лучше. Там, где можно избегать классов лучше их избегать. И здесь гораздо лучшим решением, чем определять на каждый случай свой newtype, будет определить один newtype с фантомным типом-параметром, то есть `UUID K` и вместо этого `K` подставлять различные тэги, которые имеют смысл только в этом контексте. То есть ты определишь `data C1`, `data C2`, `data C3` - это пустые типы данных, которые ты можешь инстанциировать в этот `K`. То есть у тебя будет `UUID C1`, `UUID C2`. И, если у тебя функция ожидает `UUID C1`, а ты туда передаёшь `UUID C2`, то, соответственно у тебя будет простая и понятная ошибка. Ошибка будет говорить: "ожидал C1, получил C2", а не будет говорить - вот, я ожидал там какой-то хитрый инстанс. Не, с ньютайпами там тоже будет простая ошибка.

ДР: С ньютайпами простейшие ошибки тоже будут.

РЧ: Да, да. Но это гораздо проще и красивее. Единственное что, вопрос в том, каким кайндом сделать эту переменную. И красивее всего будет сделать так, чтобы это был свой, собственный кайнд, какой-нибудь `UUID Context`. Потому что иначе это либо звёздочки и потенциально ты туда можешь запихать всё, что угодно...

ДР: Либо тебе констрейнт надо вешать.

РЧ: Э-э... Констрейнт? На что вешать?

ДР: На один из аргументов. То есть ты не сможешь описать это на уровне кайндов, но ты сможешь описать это на уровне конструкторов.

РЧ: Ну, это всё очень заморочено и вот эти открытые кайнды, это самое простое и красивое решение.

ДР: Я чего-то из твоего примера не понял, почему это решение красивое и зачем оно нам нужно. 

РЧ: Ну красиво же. Ты определяешь переменную какого-то кайнда. Ну красиво, ну Денис.

ДР: Ну это же тайпклассы на уровне кайндов.

РЧ: Нет, это не тайпклассы! Не, не!

ДР: Я понимаю, что это не тайпклассы, но это даёт какой-то похожий механизм, когда ты хочешь часть типов ограничить каким-то кайндом. (неразб.) Я не очень понимаю, как это потом всё хозйство варить.

РЧ: Что ты имеешь в виду варить?

ДР: Ну, куча фичей, которые приближаются в новый релиз GHC. У меня порой возникает ощущение, что может получиться какая-то каша.

РЧ: По-моему наоборот, это всё идёт к такой унификации. Сейчас есть какие-то частные случаи и есть какие-то кучи исключений. Наример, кайнды есть только закрытые, а открытых нет. Так же, как классы есть только открытые, а закрытых нет.

ДР: Ты имеешь в виду data kinds закрытые?

РЧ: Да, конечно. Есть, опять таки, открытые кайнды, но они только специальные. Какая-нибудь звёздочка - это открытый кайнд, потом есть кайнд решётка, который что-то посредине, как бы и не открытый и не замкнутый.

ДР: Он просто волшебный!

РЧ: Да, он, в каком-то смысле, волшебный. И сейчас просто пытаются залатать эти все дыры. Вот почему у нас есть такая штука и нет вот такой штуки? Пытаются добавить, чтобы оно как-то было красиво со всех сторон. Чтобы, если есть концепция кайнда, значит логично, что бывают открыте кайнды и есть закрытые кайнды. Если есть классы, то бывают открытые классы, бывают закрытые классы. Тогда будет гораздо меньше исключений.

ДР: А если совсем логично подойти, то нам не нужны вообще кайнды.

РЧ: А их и нет :) В 8.0 у нас будут только типы.

ДР: Про это можно рассуждать, как про сорты.

РЧ: Что ты подразумеваешь под сортами?

ДР: Смотри, у тебя есть термы, у тебя есть типы, которые описывают термы.

ДШ: А есть типы типов.

ДР: ... у тебя есть кайнды, которые описывают типы. В языках с зависимыми типами, теорем-пруверах, в том же Coq, каждый из этих слоёв называется сортом.

РЧ: Да, это есть.

ДР: Вот, и для того, чтобы дать возможность тебе описать все эти конструкции с зависимыми типами, тебе нужно бесконечное количество сортов. Потенциально.

РЧ: Смотри, почему мне не нравится здесь слово "сорты". Сорты используются в такой метатеории, где действительно надо абстрагироваться. Вот есть там типы, ести типы типов. Вот сорты, они ж откуда? Они пришли из универсальной алгебры, где у тебя вообще это самая-самая базовая теория, где есть термы и есть сорты и у сортов вообще нет никакой семантики. Поэтому это такое, более усложнённое понятие.

ДР: А с точки зрения пользователя это всё равно, пользователь на том же Coq'е не видит эти сорты, если не хочет. Он видит тайп и всё.

РЧ: Ну здесь у тебя очень простая система получается. Есть термы. У термов типы - это типы и у типов типы - это тоже типы :)

ДР: Ну, примерно тоже самое. Только если мы в Coq'е копнём поглубже то у типов тоже типы - типы. Но, если ты там включишь специальный вывод, то тебе скажут, что у типов 1 на само деле типы типов 2, а у типов 17 на самом деле типы 18. Там есть множество вселенных и там очень смешные ошибки бывают на эту тему, что Inconsistent Universe.

РЧ: Но в Хаскеле всего этого нет, поэтому можно расслабиться. Хорошо друзья, было весело, всем спасибо. С вами были Денис Шевченко, Денис Редозубов, и я, Роман Чепляка.

  

(39:30)
